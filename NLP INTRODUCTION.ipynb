{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e341ec19",
   "metadata": {},
   "source": [
    "# BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a9b5199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This': 1\n",
      "'is': 2\n",
      "'a': 3\n",
      "'simple': 1\n",
      "'example': 1\n",
      "'of': 3\n",
      "'Bag': 2\n",
      "'Words': 2\n",
      "'.': 2\n",
      "'basic': 1\n",
      "'text': 1\n",
      "'analysis': 1\n",
      "'technique': 1\n",
      "The count of 'a' is: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RTC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('punkt')  # Download the punkt tokenizer data if not already downloaded\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a simple example of a Bag of Words. Bag of Words is a basic text analysis technique.\"\n",
    "\n",
    "# Preprocess the text\n",
    "# Convert to lowercase and split into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Create a bag of words using Counter\n",
    "bag_of_words = Counter(words)\n",
    "\n",
    "# Display the Bag of Words\n",
    "for word, count in bag_of_words.items():\n",
    "    print(f\"'{word}': {count}\")\n",
    "\n",
    "# Access the count of a specific word\n",
    "word_to_count = \"a\"\n",
    "print(f\"The count of '{word_to_count}' is: {bag_of_words[word_to_count]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f47dfd",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32cb8bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RTC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words:\n",
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.', 'In', 'Python', ',', 'you', 'can', 'use', 'libraries', 'like', 'NLTK', 'or', 'spaCy', 'for', 'tokenization', '.']\n",
      "\n",
      "Tokenized Sentences:\n",
      "['Tokenization is the process of breaking text into individual words or tokens.', 'In Python, you can use libraries like NLTK or spaCy for tokenization.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the punkt tokenizer data if not already downloaded\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Tokenization is the process of breaking text into individual words or tokens. In Python, you can use libraries like NLTK or spaCy for tokenization.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Display the tokenized words and sentences\n",
    "print(\"Tokenized Words:\")\n",
    "print(words)\n",
    "\n",
    "print(\"\\nTokenized Sentences:\")\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877026a1",
   "metadata": {},
   "source": [
    "# Word Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0142f62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fli', 'happili', 'stem', 'jump']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example words to be stemmed\n",
    "words = [\"running\", \"flies\", \"happily\", \"stemming\", \"jumps\"]\n",
    "\n",
    "# Stem each word and print the results\n",
    "stemmed_word = []\n",
    "for word in words:\n",
    "    stemmed_word.append(stemmer.stem(word))\n",
    "print(stemmed_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b7642",
   "metadata": {},
   "source": [
    "# Word Lemettizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5603b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RTC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized: ['running', 'fly', 'happily', 'stemming', 'jump']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words to be lemmatized\n",
    "words = [\"running\", \"flies\", \"happily\", \"stemming\", \"jumps\"]\n",
    "\n",
    "# Lemmatize each word and print the results\n",
    "\n",
    "lemmatized_word =[] \n",
    "for word in words:\n",
    "    lemmatized_word.append(lemmatizer.lemmatize(word))\n",
    "print(f\"Lemmatized: {lemmatized_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f58d23",
   "metadata": {},
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc941a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "This is an example sentence with some stop words.\n",
      "\n",
      "Text after removing stop words:\n",
      "['example', 'sentence', 'stop', 'words', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RTC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords dataset (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Example text to remove stop words from\n",
    "text = \"This is an example sentence with some stop words.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_words = [] \n",
    "for word in words:\n",
    "    if word.lower() not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "        \n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "print(\"\\nText after removing stop words:\")\n",
    "print(filtered_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac98a6",
   "metadata": {},
   "source": [
    "# Stemming Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52744d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original paragraph:\n",
      "Natural language processing (NLP) is a field of artificial intelligence that deals with the interaction between computers and humans through natural language. It involves various tasks such as text classification, sentiment analysis, and information retrieval. NLP has applications in many domains, including healthcare, finance, and customer service.\n",
      "\n",
      "Paragraph with stemming and stop words removed:\n",
      "natur languag process ( nlp ) field artifici intellig deal interact comput human natur languag . involv variou task text classif , sentiment analysi , inform retriev . nlp ha applic mani domain , includ healthcar , financ , custom servic .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RTC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download the stopwords dataset and initialize the Porter Stemmer (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Example paragraph\n",
    "paragraph = \"Natural language processing (NLP) is a field of artificial intelligence that deals with the interaction between computers and humans through natural language. It involves various tasks such as text classification, sentiment analysis, and information retrieval. NLP has applications in many domains, including healthcare, finance, and customer service.\"\n",
    "\n",
    "# Tokenize the paragraph into sentences\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "# Process each sentence\n",
    "processed_sentences = []\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    filtered_words = [word for word in stemmed_words if word.lower() not in stop_words]\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "    processed_sentences.append(filtered_sentence)\n",
    "\n",
    "# Reconstruct the paragraph with stemming and stop words removed\n",
    "filtered_paragraph = ' '.join(processed_sentences)\n",
    "\n",
    "print(\"Original paragraph:\")\n",
    "print(paragraph)\n",
    "print(\"\\nParagraph with stemming and stop words removed:\")\n",
    "print(filtered_paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f36455",
   "metadata": {},
   "source": [
    "# Lemmetizing Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f45fa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original paragraph:\n",
      "Natural language processing (NLP) is a field of artificial intelligence that deals with the interaction between computers and humans through natural language. It involves various tasks such as text classification, sentiment analysis, and information retrieval. NLP has applications in many domains, including healthcare, finance, and customer service.\n",
      "\n",
      "Paragraph with sentence lemmatization and stop words removed:\n",
      "Natural language processing ( NLP ) field artificial intelligence deal interaction computer human natural language . involves various task text classification , sentiment analysis , information retrieval . NLP application many domain , including healthcare , finance , customer service .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RTC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RTC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RTC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Download necessary datasets and initialize the WordNet Lemmatizer (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Example paragraph\n",
    "paragraph = \"Natural language processing (NLP) is a field of artificial intelligence that deals with the interaction between computers and humans through natural language. It involves various tasks such as text classification, sentiment analysis, and information retrieval. NLP has applications in many domains, including healthcare, finance, and customer service.\"\n",
    "\n",
    "# Tokenize the paragraph into sentences\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "# Process each sentence\n",
    "processed_sentences = []\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]\n",
    "    lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "    processed_sentences.append(lemmatized_sentence)\n",
    "\n",
    "# Reconstruct the paragraph with lemmatized and stop words removed\n",
    "processed_paragraph = ' '.join(processed_sentences)\n",
    "\n",
    "print(\"Original paragraph:\")\n",
    "print(paragraph)\n",
    "print(\"\\nParagraph with sentence lemmatization and stop words removed:\")\n",
    "print(processed_paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bdc6bf",
   "metadata": {},
   "source": [
    "# TF IDF USing sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "361399c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: This is the first document.\n",
      "and: 0.0\n",
      "document: 0.46979138557992045\n",
      "first: 0.5802858236844359\n",
      "is: 0.38408524091481483\n",
      "one: 0.0\n",
      "second: 0.0\n",
      "the: 0.38408524091481483\n",
      "third: 0.0\n",
      "this: 0.38408524091481483\n",
      "\n",
      "\n",
      "Document 2: This document is the second document.\n",
      "and: 0.0\n",
      "document: 0.6876235979836938\n",
      "first: 0.0\n",
      "is: 0.281088674033753\n",
      "one: 0.0\n",
      "second: 0.5386476208856763\n",
      "the: 0.281088674033753\n",
      "third: 0.0\n",
      "this: 0.281088674033753\n",
      "\n",
      "\n",
      "Document 3: And this is the third one.\n",
      "and: 0.511848512707169\n",
      "document: 0.0\n",
      "first: 0.0\n",
      "is: 0.267103787642168\n",
      "one: 0.511848512707169\n",
      "second: 0.0\n",
      "the: 0.267103787642168\n",
      "third: 0.511848512707169\n",
      "this: 0.267103787642168\n",
      "\n",
      "\n",
      "Document 4: Is this the first document?\n",
      "and: 0.0\n",
      "document: 0.46979138557992045\n",
      "first: 0.5802858236844359\n",
      "is: 0.38408524091481483\n",
      "one: 0.0\n",
      "second: 0.0\n",
      "the: 0.38408524091481483\n",
      "third: 0.0\n",
      "this: 0.38408524091481483\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (words) and TF-IDF values\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf_matrix.toarray()\n",
    "\n",
    "# Print the TF-IDF values for each word in each document\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i + 1}: {document}\")\n",
    "    for j, word in enumerate(feature_names):\n",
    "        print(f\"{word}: {tfidf_values[i][j]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945160e",
   "metadata": {},
   "source": [
    "# TF IDF USING NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d8d0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for Document 1:\n",
      "   first: 0.34657359027997264\n",
      "   document: 0.14384103622589042\n",
      "TF-IDF for Document 2:\n",
      "   document: 0.19178804830118723\n",
      "   second: 0.46209812037329684\n",
      "TF-IDF for Document 3:\n",
      "   third: 0.6931471805599453\n",
      "   one: 0.6931471805599453\n",
      "TF-IDF for Document 4:\n",
      "   first: 0.34657359027997264\n",
      "   document: 0.14384103622589042\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = []\n",
    "for doc in corpus:\n",
    "    tokenize=word_tokenize(doc.lower())\n",
    "    withoutStopWOrds=[]\n",
    "    for word in tokenize:\n",
    "        if(word not in stop_words and word.isalnum()):\n",
    "            withoutStopWOrds.append(word)\n",
    "            \n",
    "    tokenized_corpus.append(withoutStopWOrds)\n",
    "    \n",
    "\n",
    "\n",
    "# Calculate term frequency (TF) for each document\n",
    "tf = []\n",
    "for doc in tokenized_corpus:\n",
    "    term_freq = Counter(doc)\n",
    "    total_words = len(doc)\n",
    "    tf_doc = {}\n",
    "    for word, freq in term_freq.items():\n",
    "        tf_doc[word] = freq / total_words\n",
    "    tf.append(tf_doc)\n",
    "\n",
    "# Calculate document frequency (DF)\n",
    "df={}\n",
    "\n",
    "for doc in tokenized_corpus:\n",
    "    for word in set(doc):\n",
    "        if word not in df:\n",
    "            df[word]= 1\n",
    "        else:\n",
    "            df[word]+=1\n",
    "            \n",
    "\n",
    "# Calculate inverse document frequency (IDF)\n",
    "idf = {}\n",
    "for word in df:\n",
    "    idf[word] = math.log(len(corpus) / (df[word]))\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tfidf = []\n",
    "for doc in tf:\n",
    "    doc_tfidf = {}\n",
    "    for word, freq in doc.items():\n",
    "        doc_tfidf[word] = freq * idf[word]\n",
    "    tfidf.append(doc_tfidf)\n",
    "\n",
    "# Print TF-IDF values\n",
    "for i, doc in enumerate(tfidf):\n",
    "    print(f\"TF-IDF for Document {i + 1}:\")\n",
    "    for word, score in doc.items():\n",
    "        print(f\"   {word}: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
